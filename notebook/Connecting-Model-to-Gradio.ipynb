{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting Model to Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Image classification (pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Setting up the Image Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Defining a predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Download human-readable labels for ImageNet.\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "labels = response.text.split(\"\\n\")\n",
    "\n",
    "def predict(inp):\n",
    "  inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
    "  with torch.no_grad():\n",
    "    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
    "    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}    \n",
    "  return confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Creating a Gradio Interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(fn=predict, \n",
    "             inputs=gr.Image(type=\"pil\"),\n",
    "             outputs=gr.Label(num_top_classes=3),\n",
    "             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Image Segmentation : see it in colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. AnimeGAN image transformation demo: see it in colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neon_tts_plugin_coqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import neon_tts_plugin_coqui\n",
    "from neon_tts_plugin_coqui import CoquiTTS\n",
    "\n",
    "LANGUAGES = list(CoquiTTS.langs.keys())\n",
    "coquiTTS = CoquiTTS()\n",
    "\n",
    "def tts(text: str, language: str):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n",
    "        coquiTTS.get_tts(text, fp, speaker = {\"language\" : language})\n",
    "        return fp.name\n",
    "\n",
    "inputs = [gr.Textbox(label=\"Input\", value=CoquiTTS.langs[\"en\"][\"sentence\"], max_lines=3), \n",
    "            gr.Radio(label=\"Language\", choices=LANGUAGES, value=\"en\")]\n",
    "outputs = gr.Audio(label=\"Output\")\n",
    "\n",
    "demo = gr.Interface(fn=tts, inputs=inputs, outputs=outputs)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. speach to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# save your HF API token from https:/hf.co/settings/tokens as an env variable to avoid rate limiting\n",
    "auth_token = os.getenv(\"auth_token\")\n",
    "\n",
    "# automatically load the interface from a HF model \n",
    "# you can remove the api_key parameter if you don't care about rate limiting. \n",
    "demo = gr.load(\n",
    "    \"huggingface/facebook/wav2vec2-base-960h\",\n",
    "    title=\"Speech-to-text\",\n",
    "    inputs=\"mic\",\n",
    "    description=\"Let me try to guess what you're saying!\",\n",
    "    hf_token=auth_token\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def generate(text):\n",
    "    result = generator(text, max_length=30, num_return_sequences=1)\n",
    "    return result[0][\"generated_text\"]\n",
    "\n",
    "examples = [\n",
    "    [\"The Moon's orbit around Earth has\"],\n",
    "    [\"The smooth Borealis basin in the Northern Hemisphere covers 40%\"],\n",
    "]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=gr.inputs.Textbox(lines=5, label=\"Input Text\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
